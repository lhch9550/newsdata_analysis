# BERTSum 뉴스기사 요약 모델

## 1. BERTSum 모델 개요
BERTSum 모델은 BERT의 구조 위에 두 개의 inter-sentence Transformer 레이어를 추가한 형태로 설계되었습니다. 이를 최적화하여 BertSumExt 요약 모델로 활용할 수 있습니다. 
Pre-trained BERT를 문서 요약(task-specific) 모델로 활용하기 위해서는 여러 개의 문장을 하나의 입력으로 처리할 수 있어야 하며, 각 문장에 대한 개별적인 정보를 효과적으로 추출할 수 있도록 입력 형식을 조정해야 합니다. 

BERTSum은 입력 문서의 각 문장 앞에 [CLS] 토큰을 삽입하고, 문장마다 고유한 segment embeddings을 부여하는 **interval segment embeddings** 기법을 적용합니다. 이를 통해 BERT가 문서 내 개별 문장의 관계를 보다 정교하게 학습할 수 있도록 합니다. 기존 BERT는 MLM(Masked Language Model) 방식으로 훈련되었기 때문에 출력 벡터가 토큰 단위로 생성됩니다. 이러한 한계를 극복하고자 요약 태스크에서는 **문장 수준의 표현을 다룰 수 있도록 BERT의 입력 데이터 형태를 수정**하여 사용합니다.

## 2. 모델 적용 및 학습
- **한국언론진흥재단(KPF)에서 구축한 뉴스기사 말뭉치로 학습한 KPF-BERT**를 사용
- 모델은 [빅카인즈 깃허브](https://github.com/KPFBERT/kpfbert)에서 다운로드 가능 
- 파인튜닝에 필요한 한국어 데이터셋은 **AI-HUB에서 제공하는 문서 요약 텍스트**를 활용 

## 3. 실험 방법
### 데이터셋 업로드 및 전처리
- AI-HUB 문서요약 데이터셋을 **뉴스 기사 데이터셋 형식에 맞춰 변환**

### 문장 인코딩
- **KPF-BERT 토크나이저 사용**
- PreSumm 방식을 이용하여 문장 인코딩 진행

### 모델 학습 및 예측
- 사전 훈련된 KPF-BERT 모델을 기반으로 **문장 추출 모델 생성**
- 후처리 레이어를 추가하여 문장 중요도를 평가
- **PyTorch Lightning**을 이용하여 학습 진행
- 학습된 모델을 활용하여 기사 내에서 **가장 중요한 3개 문장을 자동 추출**

## 4. 기대사항
- BERTSum 모델을 한국어 뉴스 요약에 적용함으로써, AI-HUB 및 KPF 데이터셋을 활용한 문장 추출 기반 요약 모델을 구축할 수 있습니다. 이를 통해 뉴스 기사에서 핵심 내용을 자동으로 요약할 수 있으며, 향후 다양한 뉴스 요약 및 자연어 처리 태스크에 응용할 수 있을 것으로 기대됩니다.
- 또한, 모델의 성능을 향상시키기 위한 추가적인 연구와 개선을 통해 보다 정교한 요약 결과를 도출할 수 있을 것입니다.

## 5. 참고 자료
- [관련 논문] (https://arxiv.org/abs/1908.08345)

