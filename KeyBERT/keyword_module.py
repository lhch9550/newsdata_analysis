import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import BertModel, BertTokenizer
from kiwipiepy import Kiwi

# Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
kiwi = Kiwi()

# ì‚¬ìš©ì ì •ì˜ BERT ëª¨ë¸ ë¡œë“œ
model_name_or_path = "/home/lhch9550/ê³µëª¨ì „"  # ì‚¬ìš©ìì˜ BERT ëª¨ë¸ ê²½ë¡œ
custom_model = BertModel.from_pretrained(model_name_or_path, add_pooling_layer=False)
custom_tokenizer = BertTokenizer.from_pretrained(model_name_or_path)

def encode_text(text):
    """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ (2D ë°°ì—´ë¡œ ë³€í™˜)"""
    inputs = custom_tokenizer(text, 
                              return_tensors="pt", 
                              padding=True, 
                              truncation=True,
                              max_length=512)
    outputs = custom_model(**inputs)
    embeddings = outputs.last_hidden_state.mean(dim=1)  # ğŸ”¹ í‰ê·  í’€ë§
    return embeddings.detach().numpy().reshape(1, -1)  # ğŸ”¹ 2D ë°°ì—´ë¡œ ë³€í™˜ (1, hidden_size)

def keyword_ext(text):
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    
    # ì˜ˆì™¸ ì²˜ë¦¬: ì…ë ¥ì´ ë¹„ì–´ ìˆìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    if not text or not isinstance(text, str) or text.strip() == "":
        return []

    # Kiwië¥¼ í™œìš©í•œ í˜•íƒœì†Œ ë¶„ì„ (ëª…ì‚¬ ì¶”ì¶œ)
    tokenized_doc = kiwi.tokenize(text)
    tokenized_nouns = ' '.join([word.form for word in tokenized_doc if word.tag in ['NNG', 'NNP']])

    # ì˜ˆì™¸ ì²˜ë¦¬: ëª…ì‚¬ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    if not tokenized_nouns.strip():
        return []

    # CountVectorizerë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— ë‹¨ì–´ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    if len(tokenized_nouns.split()) == 0:
        return []

    # í‚¤ì›Œë“œ í›„ë³´êµ° ìƒì„± (1-gram ê¸°ì¤€)
    try:
        count = CountVectorizer(ngram_range=(1, 1))
        count.fit([tokenized_nouns])
        candidates = count.get_feature_names_out()
    except ValueError:  # ë¹ˆ ë‹¨ì–´ ëª©ë¡ì´ë©´ ì˜ˆì™¸ ë°œìƒ
        return []

    # ì˜ˆì™¸ ì²˜ë¦¬: í‚¤ì›Œë“œ í›„ë³´ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    if len(candidates) == 0:
        return []

    # ë¬¸ì„œ ë° í‚¤ì›Œë“œ í›„ë³´êµ°ì„ ë²¡í„°ë¡œ ë³€í™˜ (2D ë°°ì—´ ìœ ì§€)
    doc_embedding = encode_text(text)
    candidate_embeddings = np.vstack([encode_text(candidate) for candidate in candidates]) if len(candidates) > 0 else np.array([])

    # ì˜ˆì™¸ ì²˜ë¦¬: ì„ë² ë”©ì´ ë¹„ì–´ ìˆìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    if candidate_embeddings.size == 0 or doc_embedding.size == 0:
        return []

    return mmr(doc_embedding, candidate_embeddings, candidates, top_n=20, diversity=0.2)

def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):
    """MMR ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•˜ì—¬ ìµœì ì˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""

    # ì˜ˆì™¸ ì²˜ë¦¬: ë¹ˆ ë°°ì—´ì¼ ê²½ìš° ì¦‰ì‹œ ë°˜í™˜
    if candidate_embeddings.shape[0] == 0:
        return []

    # ë¬¸ì„œì™€ ê° í‚¤ì›Œë“œì˜ ìœ ì‚¬ë„ ê³„ì‚°
    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)

    # í‚¤ì›Œë“œ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
    word_similarity = cosine_similarity(candidate_embeddings)

    # ì˜ˆì™¸ ì²˜ë¦¬: ìœ ì‚¬ë„ ê°’ì´ ë¹„ì–´ ìˆìœ¼ë©´ ë°˜í™˜
    if word_doc_similarity.size == 0:
        return []

    keywords_idx = [np.argmax(word_doc_similarity)]
    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]

    # ë‚˜ë¨¸ì§€ í‚¤ì›Œë“œ ì„ ì • (top_n ê°œìˆ˜ë§Œí¼ ë°˜ë³µ)
    for _ in range(top_n - 1):
        if len(candidates_idx) == 0:
            break

        candidate_similarities = word_doc_similarity[candidates_idx, :]
        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)

        mmr = (1 - diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)
        mmr_idx = candidates_idx[np.argmax(mmr)]

        # ğŸ”¹ ì—…ë°ì´íŠ¸
        keywords_idx.append(mmr_idx)
        candidates_idx.remove(mmr_idx)

    return [words[idx] for idx in keywords_idx]

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    text = """
    í•œë•ìˆ˜ êµ­ë¬´ì´ë¦¬ëŠ” 30ì¼ ë‚œë°©ë¹„ ì¸ìƒ ë“±ê³¼ ê´€ë ¨í•´ ì‹œì¥ ìƒí™©ì— ë§ê²Œ ê°€ê²©ì„ ì¡°ì •í•˜ì§€ ì•ŠëŠ” ì •ì±…ì€ 'í¬í“°ë¦¬ì¦˜'ì´ë¼ê³  ê·œì •í–ˆë‹¤.

    í•œ ì´ë¦¬ëŠ” ì´ë‚  ì˜¤ì „ ì •ë¶€ì„œìš¸ì²­ì‚¬ì—ì„œ ì£¼ì¬í•œ êµ­ë¬´íšŒì˜ ëª¨ë‘ë°œì–¸ì—ì„œ "í•œíŒŒì™€ ê°€ìŠ¤ë¹„ ë“± ê³µê³µìš”ê¸ˆ ì¸ìƒì´ ê²¹ì³ êµ­ë¯¼ë“¤ì´ ëŠë¼ëŠ” ê³ í†µì— ë§ˆìŒì´ ë¬´ê²ë‹¤"ê³  ë§í–ˆë‹¤.
    
    ì´ì–´ "ê·¸ëŸ¬ë‚˜ êµ­ë¯¼ë“¤ì´ ë¶ˆí¸í•´í•œë‹¤ê³  í•´ì„œ ì¥ê¸°ê°„ ì¡°ì •í•´ì•¼ í•  ê°€ê²©ì„ ì‹œì¥ì— ë§ì„œ ì¡°ì •í•˜ì§€ ì•Šê³  ì–µëˆ„ë¥´ëŠ” ì •ì±…ì€, ì¶”í›„ êµ­ë¯¼ë“¤ê»˜ ë” í° ë¶€ë‹´ì„ ë“œë¦¬ê³  ìš°ë¦¬ ê²½ì œì— ì•…ì˜í–¥ì„ ë¼ì¹˜ëŠ” í¬í“°ë¦¬ì¦˜ ì •ì±…ì— ë‹¤ë¦„ ì•„ë‹ˆë‹¤"ë¼ê³  ì§€ì í–ˆë‹¤.
    """
    
    print(keyword_ext(text))



